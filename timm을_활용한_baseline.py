# -*- coding: utf-8 -*-
"""timm을_활용한 Baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ETfMF3j5QfEErZUv2Zv8686RyFIuYfUT

# Import
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.optim as optim

from torch.utils.data import Dataset, DataLoader

from torchvision.transforms import Compose, ToPILImage, Resize, ToTensor, Normalize

import timm

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold

from tqdm import tqdm

"""# Config / Hyperparams"""

N_EPOCHS = 10
BATCH_SIZE = 16
BASE_LR = 1e-4
MAX_LR = 1e-3
N_FOLDS = 10
SEED = 42

"""# Data Loading"""

train = pd.read_csv('./data/train.csv')
test = pd.read_csv('./data/test.csv')

encoder = LabelEncoder()
train['label'] = encoder.fit_transform(train['label'])

skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)

for train_idx, valid_idx in skf.split(train.iloc[:, 2:], train['label']):
    break

"""# Datasets & DataLoaders"""

class CustomDataset(Dataset):
    def __init__(self, pixel_df, label_df=None, transform=None):
        self.pixel_df = pixel_df.reset_index(drop=True)
        self.label_df = label_df.reset_index(drop=True) if label_df is not None else None
        self.transform = transform

    def __len__(self):
        return len(self.pixel_df)

    def __getitem__(self, idx):
        # Reshape to (32, 32) from flattened data
        image = self.pixel_df.iloc[idx].values.astype(np.uint8).reshape(32, 32)
        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # shape: (1, 32, 32)

        if self.transform:
            image = self.transform(image)

        if self.label_df is not None:
            label = torch.tensor(self.label_df.iloc[idx], dtype=torch.long)
            return image, label
        else:
            return image

train_transform = Compose([
    ToPILImage(),
    Resize((224, 224)),
    ToTensor(),
    Normalize(mean=[0.5], std=[0.5]),
])

train_dataset = CustomDataset(pixel_df=train.iloc[train_idx, 2:], label_df=train.iloc[train_idx, 1], transform=train_transform)
valid_dataset = CustomDataset(pixel_df=train.iloc[valid_idx, 2:], label_df=train.iloc[valid_idx, 1], transform=train_transform)
test_dataset = CustomDataset(pixel_df=test.iloc[:, 1:], transform=train_transform)

loader_params = {
    'batch_size': BATCH_SIZE,
    'num_workers': 8,
    'pin_memory': True
}

train_loader = DataLoader(train_dataset, shuffle=True, **loader_params)
valid_loader = DataLoader(valid_dataset, shuffle=False, **loader_params)
test_loader = DataLoader(test_dataset, shuffle=False, **loader_params)

"""# Model & Optim Setup"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 앙상블을 위한 여러 모델 정의
models = {
    'efficientnet_b0': timm.create_model(
        model_name="tf_efficientnet_b0.ns_jft_in1k",
        pretrained=False,
        num_classes=10,
        in_chans=1
    ).to(device),
    'efficientnet_b4': timm.create_model(
        model_name="tf_efficientnet_b4.ns_jft_in1k",
        pretrained=False,
        num_classes=10,
        in_chans=1
    ).to(device),
    'resnet50': timm.create_model(
        model_name="resnet50.a1_in1k",
        pretrained=False,
        num_classes=10,
        in_chans=1
    ).to(device)
}

# 각 모델별 옵티마이저와 스케줄러 설정
optimizers = {name: optim.AdamW(model.parameters(), lr=BASE_LR) for name, model in models.items()}
schedulers = {
    name: optim.lr_scheduler.OneCycleLR(
        opt,
        max_lr=MAX_LR,
        epochs=N_EPOCHS,
        steps_per_epoch=len(train_loader),
        pct_start=0.3,
        div_factor=25,
        final_div_factor=1e4
    ) for name, opt in optimizers.items()
}

criterion = nn.CrossEntropyLoss()

"""# Training Loop"""

def train_one_epoch(models, loader, criterion, optimizers, schedulers, device):
    for model in models.values():
        model.train()
    
    running_loss = 0.0
    total_batches = 0

    for images, labels in tqdm(loader, desc="Training", leave=False):
        images, labels = images.to(device), labels.to(device)
        
        # 각 모델별로 학습
        for name, model in models.items():
            optimizers[name].zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizers[name].step()
            schedulers[name].step()
            running_loss += loss.item()
        
        total_batches += 1

    epoch_loss = running_loss / (total_batches * len(models))
    return epoch_loss

def validate_one_epoch(models, loader, criterion, device):
    for model in models.values():
        model.eval()
    
    running_loss = 0.0
    all_predictions = []
    all_labels = []
    total_batches = 0

    with torch.no_grad():
        for images, labels in tqdm(loader, desc="Validation", leave=False):
            images, labels = images.to(device), labels.to(device)
            
            # 각 모델의 예측을 저장
            model_predictions = []
            for model in models.values():
                outputs = model(images)
                loss = criterion(outputs, labels)
                running_loss += loss.item()
                model_predictions.append(outputs)
            
            # 앙상블 예측 (평균)
            ensemble_outputs = torch.stack(model_predictions).mean(0)
            _, predicted = torch.max(ensemble_outputs.data, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            total_batches += 1

    epoch_loss = running_loss / (total_batches * len(models))
    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))
    return epoch_loss, accuracy

if __name__ == '__main__':
    best_loss = float('inf')
    best_models = None
    
    for epoch in range(N_EPOCHS):
        print(f"\nEpoch [{epoch+1}/{N_EPOCHS}]")
        
        # 현재 학습률 출력
        for name, scheduler in schedulers.items():
            current_lr = scheduler.get_last_lr()[0]
            print(f"{name} Learning Rate: {current_lr:.2e}")

        # Train
        train_loss = train_one_epoch(models, train_loader, criterion, optimizers, schedulers, device)

        # Validate
        val_loss, val_acc = validate_one_epoch(models, valid_loader, criterion, device)

        print(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc*100:.2f}%")

        # Check for best models
        if val_loss < best_loss:
            best_loss = val_loss
            best_models = {name: model.state_dict() for name, model in models.items()}

    # Load best models
    for name, state_dict in best_models.items():
        models[name].load_state_dict(state_dict)

    # Inference
    for model in models.values():
        model.eval()
    
    preds = []

    with torch.no_grad():
        for images in tqdm(test_loader, desc="Inference", leave=False):
            images = images.to(device)
            
            # 각 모델의 예측을 저장
            model_predictions = []
            for model in models.values():
                outputs = model(images)
                model_predictions.append(outputs)
            
            # 앙상블 예측 (평균)
            ensemble_outputs = torch.stack(model_predictions).mean(0)
            _, predicted = torch.max(ensemble_outputs.data, 1)
            preds.extend(predicted.cpu().numpy())

    # Decode predictions
    pred_labels = encoder.inverse_transform(preds)

    # Submission
    submission = pd.read_csv('./data/sample_submission.csv')
    submission['label'] = pred_labels
    submission.to_csv('ensemble_submission.csv', index=False)